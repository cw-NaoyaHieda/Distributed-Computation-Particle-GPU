---
title: "Intoroducing Monte Carlo Methods with R   (Part2  Monte Carlo integration)"
author: "Naoya Hieda"
date: "2017年5月9日"
output:
  html_document:
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
library(ggplot2)
library(reshape2)
library(dplyr)
library(MASS)
theme_set(theme_bw())
```

ParticleFilterに到達する前の重点サンプリングやPMCMC(Particle Marcoh chain monte carlo)のもとになっているMCMCの理論について  
いろいろと怪しい部分があったのでまとめる。  
このMarkdownは主に三章のモンテカルロ積分関係について

## 参考文献

[Rによるモンテカルロ法入門](https://pub.maruzen.co.jp/book_magazine/book_data/search/9784621065273.html)  
[人工知能に関する断創録](http://aidiary.hatenablog.com/entry/20140620/1403272044)  
[Wolfeyes Bioinformatics beta](http://yagays.github.io/blog/2012/10/20/archive-introducing-monte-carlo-methods-with-r/)

Rにもともとarea関数とintegrate関数が存在する。ただし、area()は積分で無限の範囲を扱えないし、integrateも安定性に乏しい  
実験として以下の積分について、integrate関数と実際の値を比較してみる(下記を積分すると$\Gamma$の対数
$$
\int_{0}^{\infty} x^{\lambda-1}exp(-x)dx
$$

```{r}
ch <- function(la){
  integrate(function(x) {x^{la-1}*exp(-x)}, 0, Inf)$val
}
plot_d <- data.frame(x = lgamma(seq(0.01,10,le=100)),y = log(apply(as.matrix(seq(0.01,10,le=100)), 1, ch)))
ggplot(plot_d,aes(x=x,y=y))+geom_point()
```
この場合は、結構きれい

integrate関数のような数値積分方法で困難なのは、被積分関数にいおいて重要な範囲を見逃しやすいこと。
これに対してシミュレーションでは、積分にかかわる確率密度の情報を活用することで、この範囲に絞った適用が可能。  

位置パラメータ$\theta=350$とするコーシー分布の乱数10個をサンプルとして検討する。平坦な事前分布を仮定するとサンプルの(疑似)周辺分布が以下のようになる
$$
m(x) = \int_{-\infty}^{\infty}\prod_{i=1}^{10} \frac{1}{\pi}\frac{1}{1+(x_i-\theta)^2}d\theta
$$
しかし、integrateは誤った数値を返す  
下記は誤差評価を確認した後、area関数との積分結果の対数尤度比較
```{r}
cac=rcauchy(10)+350
lik=function(the){
  u=dcauchy(cac[1]-the)
  for(i in 2:10)
    u=u*dcauchy(cac[i]-the)
    return(u)
}

print(integrate(lik,-Inf,Inf))

print(integrate(lik,200,400))

cac=rcauchy(10)

nin=function(a){integrate(lik,-a,a)$val}
nan=function(a){area(lik,-a,a)}
x=seq(1,10^3,le=10^4)
y=log(apply(as.matrix(x),1,nin))
z=log(apply(as.matrix(x),1,nan))
plot(x,y,type="l",ylim=range(cbind(y,z)),lwd=2)
ggplot()+geom_line(data = data.frame(x,y),mapping = aes(x=x,y=y))+geom_line(data = data.frame(x,z),mapping = aes(x=x,y=z),colour='blue',linetype=2)
```

## 古典的なモンテカルロ計算
シミュレーションを実際の問題に適用する前に、こうした応用が適正であることを再確認しておく。
一般に以下の積分をどのように評価するのかということが問題になる。
$$
E_{f}[h(X)] = \int_{\chi}h(x)f(x)dx
$$
ここで$\chi$は乱数Xの値の集合で、これは通常の密度fの台に等しくなる。  
これを近似するモンテカルロ法の原理は、密度fからサンプル$(X_1,\dots,X_m)$を生成して、近似として以下の経験平均を掲示すること
$$
\bar{h}_m = \frac{1}{m}\sum_{j=1}^m h(x_j)
$$

$\bar{h}_m$は大数の強法則により、ほぼ間違いなく$E_f(h(X))$に収束する。よって、Rならmeanなどを計算することで求まる。
さらに、$h^2(X)$が$f$のもとで有限の期待値をもつ場合、$\bar{h}_m$の収束時間を評価することが可能。
収束時間は$O(\sqrt{m})$となり、近似の漸近的な分散は以下となるため。
$$
var(\bar{h}_m)=\frac{1}{m}\int_{\chi}(h(x)-E_f[h(X)])^2f(x)dx
$$
これは、サンプル$(X_1,\dots,X_m)$からも以下のように推定できる。
$$
v_m = \frac{1}{m^2}\sum_{j=1}^{m}[h(x_j)-\bar{h}_m]^2
$$
さらには中心極限定理によってmが十分大きいのであれば、いかが近似的に$N(0,1)$の正規分布をする変数となる。
$$
\frac{\bar{h}_m-E_f[h(X)]}{\sqrt{v_m}}
$$


次の仮の関数で試してみる
$$
h(x)= [cos(50x)+sin(20x)]^2
$$
上が実際にこの曲線が描く概形で、下がこれの近似式の収束をしめしている
```{r}
h <- function(x){(cos(50 * x) + sin(20 * x))^2}
ggplot()+geom_line(mapping=aes(x= 0:100/100,y =h(0:100/100)))
integrate(h,0,1)
x <- h(runif(10^4))
estint <- cumsum(x)/(1:10^4)
esterr <- sqrt(cumsum((x - estint)^2))/(1:10^4)
ggplot(data.frame(estint), aes(x=1:10^4,y=estint))+geom_line()+ylim(mean(x)+20*c(-esterr[10^4],esterr[10^4]))+
  geom_line(mapping = aes(x=1:10^4,y=estint+2*esterr,color="gold"))+geom_line(mapping=aes(x=1:10^4,y=estint-2*esterr,color="gold"))
```

### 練習問題 3.1
正規・コーシーベイズ推定量
$$
\delta(x) = \int_{\infty}^{\infty} \frac{\theta}{1+\theta^2} e^{-(x-\theta)/2}d\theta/\int_{\infty}^{\infty} \frac{1}{1+\theta^2} e^{-(x-\theta)/2}d\theta
$$
- 被積分関数をプロットし、コーシー・シミュレーションにもとづくモンテカルロ積分を計算する。
```{r}
delta_numer <- function(theta){theta/(1+theta^2)*exp(-(x-theta)^2/2)}
delta_denom <- function(theta){1/(1+theta^2)*exp(-(x-theta)^2/2)}
par(mfrow=c(3,2))
for(x in c(0,2,4)){
  curve(delta_numer,from=-10,to=10,main=paste("numerator : x=",x))
  curve(delta_denom,from=-10,to=10,main=paste("denominator : x=",x))
} 

for(x in c(0,2,4)){
  #正規分布から乱数を発生させて、コーシーで評価
  N <-10^5
  norm <- rnorm(N,x)
  cauchy <- dcauchy(norm)
  print(mean(norm*cauchy)/mean(cauchy))
  #その逆　結果はほぼ同じ
  cauchy <- rcauchy(N)
  norm <- dnorm(cauchy,x)
  print(mean(cauchy*norm)/mean(norm))
} 
```
- 収束を推定値の標準誤差でモニタリングする。95\%の信頼幅を小数点3位の精度で求める
```{r}
estint <- cumsum(cauchy*norm)/cumsum(norm)
esterr <- sqrt(cumsum((cauchy-estint)^2)/c(1:length(cauchy))^2)
plot(esterr,type='l')
```



# 本題その1
# 重点サンプリング
上の近似の評価は、たいていの場合は最適ではない
## 参照量を任意に変更
重点サンプリング法は、期待値$E_f[h(x)]=\int_{\chi}h(x)f(x)dx$の代替え式に基づく。ある任意の密度gで$h\times f$がゼロとは異なり、真に正ならば、次のように書き換えることができる
$$
E_f[h(X)]=\int_{\chi}h(x)\frac{f(x)}{g(x)}g(x)dx=E_g[\frac{h(X)f(X)}{g(X)}]
$$
これは密度gの下での期待値となる。この重点サンプリング基本恒等式は、次の推定量の利用を保証する。
$$
\frac{1}{n}\sum_{j=1}^{m}\frac{f(X_j)}{g(X_j)}h(X_j)\rightarrow E_f[h(X)]
$$
これはgから生成したサンプルに基づく。このように期待値をgの下での期待値として書くことができるので、分布gの選択がなんであれ、通常のモンテカルロ推定量
$\bar{h}$が収束するのと同じように収束する。

### 練習問題 3.4
fを正規pdfとし、$h(x)をexp(-(x-3)^2)+exp(-(x-6)^2/2)$として期待値$E[h(X)]$の計算を検討する

- $E_f[h(X)]$が閉じた式で計算できることを示し、その値を導く。

- 正規分布N(0,1)に基づくサンプルサイズ$10^3$の通常のモンテカルロ近似を構成し、誤差を評価する。
```{r}
N <- 10^3
h <- function(x){exp(-(x-3)^2/2)+exp(-(x-6)^2/2)}
sample <- rnorm(N)
n_mc_est <- cumsum(h(sample))/c(1:length(sample))
n_mc_err <- sqrt(cumsum((sample - n_mc_est)^2))/(1:length(sample))
n_mc <- data.frame(est = n_mc_est,error_max = n_mc_est + 2 * n_mc_err,error_min = n_mc_est - 2 * n_mc_err)
ggplot(n_mc,aes(x=1:length(sample),y=est))+geom_line()+
  geom_line(mapping = aes(x=1:length(sample),y=error_max),colour='gold')+
  geom_line(mapping = aes(x=1:length(sample),y=error_min),colour='gold')+
  xlab('sample size')
n_mc_est[N]
```

- 一様分布u(-8,-1)に対応する重点関数gにもとづくサンプルサイズ$10^3$の重点サンプリング近似と比較する。(これは収束しない　範囲を網羅してない)  
・・・そのわりには収束してるっぽい
```{r}
sample_g <- runif(N,-8,-1)
g_mc_est <- h(sample_g) * dnorm(sample_g) / dunif(sample_g,-8,-1) /c(1:length(sample_g))
g_mc_err <- sqrt(cumsum((h(sample_g) * dnorm(sample_g) / dunif(sample_g,-8,-1) - g_mc_est)^2))/c(1:length(sample_g))
g_mc <- data.frame(est = g_mc_est,error_max = g_mc_est + 2 * g_mc_err,error_min = g_mc_est - 2 * g_mc_err)
ggplot(g_mc,aes(x=1:length(sample),y=est))+geom_line(colour='red')+
  geom_line(mapping = aes(x=1:length(sample),y=error_max),colour='gold')+
  geom_line(mapping = aes(x=1:length(sample),y=error_min),colour='gold')+
  xlab('sample size')
```

```{r}
sample_g <- runif(N,-2,10)
g_mc_est <- h(sample_g) * dnorm(sample_g) / dunif(sample_g,-2,10) /c(1:length(sample_g))
g_mc_err <- sqrt(cumsum((h(sample_g) * dnorm(sample_g) / dunif(sample_g,-2,10) - g_mc_est)^2))/c(1:length(sample_g))
g_mc <- data.frame(est = g_mc_est,error_max = g_mc_est + 2 * g_mc_err,error_min = g_mc_est - 2 * g_mc_err)
ggplot(g_mc,aes(x=1:length(sample),y=est))+geom_line(colour='red')+
  geom_line(mapping = aes(x=1:length(sample),y=error_max),colour='gold')+
  geom_line(mapping = aes(x=1:length(sample),y=error_min),colour='gold')+
  xlab('sample size')

g_mc_est[N]
```


通常のモンテカルロ合計によって裾の確率を近似する方法は、裾の外れに近づくにつれ破綻する。たとえば、$Z\sim N(0,1)$でP(Z > 4.5)の確率を知りたいとする。  
```{r}
pnorm(-4.5, log=T)
pnorm(-4.5)
```
約300万回の反復で1回しか起きない。  
きわめて稀な事象の確率を知ろうとしたとき、$f$からの単純なシミュレーションでは、安定した解をえるのに膨大なシミュレーションが必要になる。しかし、重点サンプリングのおかげで、精度を大幅に改善することができ、したがってシミュレーション回数を数桁分減らすことができる。  
例えば台が$(4.5,\infty)$に制約された分布を例にする。ここではモンテカルロ推定量の余分で不要な分散がゼロのシミュレーション(つまり $x \leq 4.5$の範囲)により消えている。gを4.5で切り詰めた指数分布の密度とするのが自然

切断指数分布（truncated exponential distribution）という分布、今は指数分布から4.5以上の区間を取り出してきて、その区間の面積が1になるように再調整した分布
$$
g(y)=e^{-y}/\int_{4.5}^{\infty}e^{-x}dx=e^{-(y-4.5)}
$$
すると対応する重点サンプリングによる裾の確率の推定量が以下のように与えられる
$$
\frac{1}{m}\sum_{i=1}^{m}\frac{f(Y^{(i)})}{g(Y^{(i)})}=\frac{1}{m}\sum_{i=1}^{m}\frac{e^{-Y_i^2-4.5}}{\sqrt{2\pi}}
$$
```{r}
Nsim <- 10^3
y <- rexp(Nsim) + 4.5
weit =dnorm(y)/dexp(y-4.5)
plot(cumsum(weit)/1:Nsim,type="l")
abline(a=pnorm(-4.5),b=0,col="red")
```
かなり早い段階で収束している。

### 練習問題3.5  
上の例で指数分布が切り詰められると、裾の確率近似の分散に影響することを調べる
```{r}
par(mfrow=c(2,2))
for(lambda in c(1, 5, 10, 20)){
  Nsim <- 10^4
  y <- rexp(Nsim)/lambda+4.5
  weit <- dnorm(y)/dexp(y-4.5, lambda)
  estint <- cumsum(weit)/1:Nsim
  esterr <- sqrt(cumsum((weit-estint)^2))/(1:Nsim)
  plot(estint, xlab="Mean and error range", ylab="prob", type="l", main=paste("lambda = ",lambda))
  lines(estint+2*esterr, col="gold", lwd=2)
  lines(estint-2*esterr, col="gold", lwd=2)
  abline(h=pnorm(-4.5), col="red")
}
```

たぶん、去年の研究の時も、VaRの計算はgの範囲を狭めてサンプリングによって求めるべきだった。


ベイズの枠組みにもとづいて、ベータ分布$Be(\alpha,\beta)$からの観測地xを考える。
$$
x\sim \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}I\hspace{-.1em}I
$$
$(\alpha,\beta)$には以下の形式の共役事前分布族がある。  
$$
\pi(\alpha,\beta)\propto \left\{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \right\}^{\lambda}x_0^{\alpha}y_0^{\beta}
$$
ここで$\lambda,x_0,y_0$はハイパーパラメータ。この際、事後分布は以下に等しい

$$
\pi(\alpha,\beta|x)\propto \left\{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\right\}^{\lambda+1}[xx_0]^{\alpha}[(1-x)y_0]^{\beta}
$$
この分布は、ガンマが扱いにくいというだけでも制御しにくい。そのため

```{r}
f <- function(a,b){
  exp(2*(lgamma(a+b) - lgamma(a) - lgamma(b)) +
    a*log(0.3) + b*log(0.2))}
  
aa = 1:150
bb = 1:100

post = outer(aa,bb,f)
image(aa,bb,post,xlab=expression(alpha),ylab="")
contour(aa,bb,post,add=T)

```


